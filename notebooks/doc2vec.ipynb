{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "IPython.notebook.set_autosave_interval(30000)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 30 seconds\n"
     ]
    }
   ],
   "source": [
    "%autosave 30\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/huawei123/kwx1991442/code-classification\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huawei123/kwx1991442/venv/lib/python3.8/site-packages/liblinear/liblinear.py:135: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def csr_to_problem_jit(l, x_val, x_ind, x_rowptr, prob_val, prob_ind, prob_rowptr):\n",
      "2023-08-25 15:38:19.067501: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-25 15:38:20.583352: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from typing import *\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from torch import Generator\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from liblinear.liblinearutil import (\n",
    "    problem, parameter, train, predict, evaluations, save_model\n",
    ")\n",
    "from src.utils import CustomSummaryWriter\n",
    "from src.baseline.dataloader import UITestsDataset\n",
    "from src.word2vec.dataloader import W2VClassificationCollator, TrainW2VModel\n",
    "from src.params import (PATH_TEST_UI, PATH_PARSED_CLASSIFUI,\n",
    "    PATH_SAVE_METRICS, PATH_SAVE_MODEL, PATH_SAVE_PREDICTIONS)\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.main import parse_args\n",
    "\n",
    "args = parse_args(True, [\"--method\", \"word2vec\", \"--exp-name\", \"DEBUGDOC2VEC\", '--tokens-source', 'origin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.mode != 'eval':\n",
    "    raise NotImplementedError()\n",
    "\n",
    "# load data\n",
    "mode = 'train'\n",
    "tokens_source = args.tokens_source\n",
    "\n",
    "if tokens_source == 'origin':\n",
    "    tests_ui_folder=PATH_TEST_UI\n",
    "elif tokens_source == 'classifui':\n",
    "    tests_ui_folder=PATH_PARSED_CLASSIFUI\n",
    "\n",
    "dataset = UITestsDataset(tests_ui_folder, mode, debug=True)\n",
    "logger.info(f\"UITestsDataset is initialized in mode {mode} from {tests_ui_folder} \"\n",
    "            f\" with {tokens_source=}. {len(dataset)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.word2vec.dataloader import TokenizerSplitter\n",
    "\n",
    "splitter = TokenizerSplitter()\n",
    "sentences_train = [splitter(text) for text, _ in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import doc2vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "tag_documents = [TaggedDocument(sentences_train[i], [i]) for i in range(len(sentences_train))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['vari', 'adic', 'Ġffi', 'Ġ6', 'Ċ', 'Ċ', '#!', '[', 'cr', 'ate', '_', 'type', '=\"', 'lib', '\"]', 'Ċ', '#!', '[', 'feature', '(', 'c', '_', 'vari', 'adic', ')]', 'Ċ', 'Ċ', 'pub', 'Ġunsafe', 'Ġextern', 'Ġ\"', 'C', '\"', 'Ġfn', 'Ġuse', '_', 'var', 'arg', '_', 'lifetime', '(', 'ĊĠĠĠ', 'Ġx', ':', 'Ġu', 'size', ',', 'ĊĠĠĠ', 'Ġy', ':', 'Ġ...', 'Ċ', ')', 'Ġ->', 'Ġ&', 'u', 'size', 'Ġ{', 'Ġ//', '~', 'ĠERROR', 'Ġmissing', 'Ġlifetime', 'Ġspecifier', 'ĊĠĠĠ', 'Ġ&', '0', 'Ċ', '}', 'Ċ', 'Ċ', 'pub', 'Ġunsafe', 'Ġextern', 'Ġ\"', 'C', '\"', 'Ġfn', 'Ġuse', '_', 'normal', '_', 'arg', '_', 'lifetime', '(', 'x', ':', 'Ġ&', 'u', 'size', ',', 'Ġy', ':', 'Ġ...)', 'Ġ->', 'Ġ&', 'u', 'size', 'Ġ{', 'Ġ//', 'ĠOK', 'ĊĠĠĠ', 'Ġx', 'Ċ', '}', 'ĊĊ', 'Ċ', 'error', '[', 'E', '01', '06', ']:', 'Ġmissing', 'Ġlifetime', 'Ġspecifier', 'ĊĠ', 'Ġ-->', 'Ġ$', 'DIR', '/', 'vari', 'adic', '-', 'ffi', '-', '6', '.', 'rs', ':', '7', ':', '6', 'ĊĠĠ', 'Ġ|', 'Ċ', 'LL', 'Ġ|', 'Ġ)', 'Ġ->', 'Ġ&', 'u', 'size', 'Ġ{', 'ĊĠĠ', 'Ġ|', 'ĠĠĠĠĠ', 'Ġ^', 'Ġexpected', 'Ġnamed', 'Ġlifetime', 'Ġparameter', 'ĊĠĠ', 'Ġ|', 'ĊĠĠ', 'Ġ=', 'Ġhelp', ':', 'Ġthis', 'Ġfunction', \"'s\", 'Ġreturn', 'Ġtype', 'Ġcontains', 'Ġa', 'Ġb', 'orrow', 'ed', 'Ġvalue', ',', 'Ġbut', 'Ġthere', 'Ġis', 'Ġno', 'Ġvalue', 'Ġfor', 'Ġit', 'Ġto', 'Ġbe', 'Ġb', 'orrow', 'ed', 'Ġfrom', 'Ċ', 'help', ':', 'Ġconsider', 'Ġusing', 'Ġthe', 'Ġ`', \"'\", 'static', '`', 'Ġlifetime', 'ĊĠĠ', 'Ġ|', 'Ċ', 'LL', 'Ġ|', 'Ġ)', 'Ġ->', 'Ġ&', \"'\", 'static', 'Ġu', 'size', 'Ġ{', 'ĊĠĠ', 'Ġ|', 'ĠĠĠĠĠĠ', 'Ġ+', '++++', '++', 'Ċ', 'Ċ', 'error', ':', 'Ġabort', 'ing', 'Ġdue', 'Ġto', 'Ġprevious', 'Ġerror', 'Ċ', 'Ċ', 'For', 'Ġmore', 'Ġinformation', 'Ġabout', 'Ġthis', 'Ġerror', ',', 'Ġtry', 'Ġ`', 'rust', 'c', 'Ġ--', 'explain', 'ĠE', '01', '06', '`.', 'Ċ'], tags=[0])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_documents[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = doc2vec.Doc2Vec(vector_size = 300, window = 1, min_count = 3, workers = 6, epochs=20)\n",
    "model.build_vocab(tag_documents)\n",
    "model.train(tag_documents, total_examples=model.corpus_count, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from src.word2vec.dataloader import LiblinearSplitter, TokenizerSplitter\n",
    "\n",
    "class D2VClassificationCollator(object):\n",
    "    d2v_model : Doc2Vec\n",
    "    labelencoder : LabelEncoder\n",
    "    splitter : Union[LiblinearSplitter, TokenizerSplitter]\n",
    "    \n",
    "    def __init__(self, \n",
    "        d2v_model : Doc2Vec,\n",
    "        splitter : Union[LiblinearSplitter, TokenizerSplitter], \n",
    "        classes : List[str]\n",
    "        ) -> None:\n",
    "        self.d2v_model = d2v_model\n",
    "        self.splitter = splitter\n",
    "        self.labelencoder = LabelEncoder().fit(classes)\n",
    "\n",
    "    def encode_sentence(self, sentence):\n",
    "        if len(sentence) == 0:\n",
    "            return np.zeros(self.w2v_model.vector_size)        \n",
    "        else:\n",
    "            return self.d2v_model.infer_vector(sentence)\n",
    "\n",
    "    def encode_labels(self, labels: List[str]) -> np.ndarray:\n",
    "        return self.labelencoder.transform(labels)\n",
    "\n",
    "    def __call__(self, rawtexts_labels : List[Tuple[str, str]]) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        rawtexts, labels = zip(*rawtexts_labels)\n",
    "        vectors = [self.encode_sentence(self.splitter(text)) for text in rawtexts]\n",
    "        encoded_labels = self.encode_labels(labels)\n",
    "        return vectors, encoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.infer_vector(sentences_train[0])\n",
    "seed = args.seed\n",
    "traintestsplit = args.traintestsplit\n",
    "lengths = [traintestsplit, 1-traintestsplit]\n",
    "\n",
    "splitgenerator = Generator().manual_seed(seed)\n",
    "train_set, val_set = random_split(dataset, \n",
    "    lengths=lengths, generator=splitgenerator)\n",
    "logger.info(f\"Split generator initialized with {seed=}, {lengths=}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn = D2VClassificationCollator(model, splitter, dataset.classes)\n",
    "X_train, Y_train = collate_fn(train_set)\n",
    "X_val,   Y_val   = collate_fn(val_set)\n",
    "logger.info(f\"{len(dataset.classes)=}, {len(X_train)=}, {len(X_val)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.save_metrics = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huawei123/kwx1991442/venv/lib/python3.8/site-packages/sklearn/model_selection/_split.py:725: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 20 is out of bounds for axis 0 with size 20",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m training\u001b[38;5;241m.\u001b[39mfit(X_train, Y_train)\n\u001b[1;32m     22\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluation of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m classifier\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m \u001b[43mtraining\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_val\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/kwx1991442/code-classification/src/word2vec/trainer.py:121\u001b[0m, in \u001b[0;36mTrainer.eval\u001b[0;34m(self, X_val, Y_val)\u001b[0m\n\u001b[1;32m    118\u001b[0m metrics \u001b[39m=\u001b[39m evaluate(Y_pred, Y_val, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabelencoder)\n\u001b[1;32m    120\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog_tensorboard(metrics)\n\u001b[0;32m--> 121\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msave_metrics(metrics)\n\u001b[1;32m    122\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_model()\n\u001b[1;32m    123\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_predictions(Y_pred, Y_val)\n",
      "File \u001b[0;32m~/kwx1991442/code-classification/src/word2vec/trainer.py:150\u001b[0m, in \u001b[0;36mTrainer.save_metrics\u001b[0;34m(self, metrics)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[39mfor\u001b[39;00m i, (count_x, count_y) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mzip\u001b[39m(m_count[\u001b[39m'\u001b[39m\u001b[39mcounts_y_pred_x\u001b[39m\u001b[39m'\u001b[39m], m_count[\u001b[39m'\u001b[39m\u001b[39mcounts_y_pred\u001b[39m\u001b[39m'\u001b[39m])):\n\u001b[1;32m    149\u001b[0m     classname \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabelencoder\u001b[39m.\u001b[39minverse_transform([count_x])[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 150\u001b[0m     f\u001b[39m.\u001b[39mwrite(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mcount_x\u001b[39m:\u001b[39;00m\u001b[39m3\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{\u001b[39;00mclassname\u001b[39m:\u001b[39;00m\u001b[39m30\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{\u001b[39;00mcount_y\u001b[39m:\u001b[39;00m\u001b[39m3\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m (out of \u001b[39m\u001b[39m{\u001b[39;00mm_count[\u001b[39m'\u001b[39m\u001b[39mcounts_y_val\u001b[39m\u001b[39m'\u001b[39m][count_x]\u001b[39m:\u001b[39;00m\u001b[39m6\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 20 is out of bounds for axis 0 with size 20"
     ]
    }
   ],
   "source": [
    "from src.word2vec.trainer import classifiers, Trainer\n",
    "\n",
    "experiment_name = f\"TESTD2V\"\n",
    "\n",
    "available_classifiers = list(classifiers.keys())\n",
    "selected_classifier = args.classifier\n",
    "logtb = True\n",
    "\n",
    "for name in available_classifiers:\n",
    "    if selected_classifier in name or selected_classifier == 'all':\n",
    "        logger.info(f\"Starting {name} classifier\")\n",
    "        training = Trainer(name, \n",
    "            logtb, \n",
    "            experiment_name, \n",
    "            labelencoder=collate_fn.labelencoder, \n",
    "            save_model=args.save_model,\n",
    "            save_metrics=args.save_metrics,\n",
    "            save_predictions=args.save_predictions\n",
    "        )\n",
    "        logger.info(f\"Fitting {name} classifier\")\n",
    "        training.fit(X_train, Y_train)\n",
    "        logger.info(f\"Evaluation of {name} classifier\")\n",
    "        training.eval(X_val, Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.word2vec.trainer import evaluate\n",
    "\n",
    "Y_pred = training.clf.best_estimator_.predict(X_val)\n",
    "metrics = evaluate(Y_pred, Y_val, collate_fn.labelencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'accuracy': 0.7450980392156863,\n",
       "  'micro_precision': 0.7450980392156863,\n",
       "  'micro_recall': 0.7450980392156863,\n",
       "  'micro_fbeta_score': 0.7450980392156863,\n",
       "  'macro_precision': 0.7450980392156863,\n",
       "  'macro_recall': 0.7450980392156863,\n",
       "  'macro_fbeta_score': 0.7450980392156863},\n",
       " {'precision': array([0.8125    , 0.5       , 1.        , 1.        , 0.33333333,\n",
       "         0.71428571, 1.        , 0.5       , 0.68965517, 0.73913043,\n",
       "         1.        , 1.        , 0.5       , 0.66666667, 0.9       ,\n",
       "         1.        , 1.        , 1.        , 0.74264706, 1.        ,\n",
       "         0.5       ]),\n",
       "  'recall': array([1.        , 1.        , 1.        , 1.        , 0.25      ,\n",
       "         0.55555556, 0.75      , 1.        , 0.86956522, 0.62962963,\n",
       "         1.        , 0.875     , 0.75      , 1.        , 0.5       ,\n",
       "         1.        , 1.        , 1.        , 0.808     , 0.6       ,\n",
       "         1.        ]),\n",
       "  'fbeta_score': array([0.89655172, 0.66666667, 1.        , 1.        , 0.28571429,\n",
       "         0.625     , 0.85714286, 0.66666667, 0.76923077, 0.68      ,\n",
       "         1.        , 0.93333333, 0.6       , 0.8       , 0.64285714,\n",
       "         1.        , 1.        , 1.        , 0.77394636, 0.75      ,\n",
       "         0.66666667])},\n",
       " {'counts_y_val_x': array([ 0,  1,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "         18, 19, 20]),\n",
       "  'counts_y_val': array([ 13,   2,   1,   4,   9,   4,   1,  23,  81,   1,   8,   4,   2,\n",
       "          18,   1,   1,   2, 125,   5,   1]),\n",
       "  'counts_y_pred_x': array([ 0,  1,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "         18, 19, 20]),\n",
       "  'counts_y_pred': array([ 16,   4,   1,   3,   7,   3,   2,  29,  69,   1,   7,   6,   3,\n",
       "          10,   1,   1,   2, 136,   3,   2])})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_vals, m_arrs, m_count = metrics\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 20)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(m_count['counts_y_val']), len(m_count['counts_y_pred_x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = collate_fn.labelencoderle\n",
    "\n",
    "for i, (count_x, count_y) in enumerate(zip(m_count['counts_y_pred_x'], m_count['counts_y_pred'])):\n",
    "    classname = f\"({le.inverse_transform([count_x])[0]})\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40f0bfdc1912eb71bf0d0b3c4b481e172567f32ca306c238f59a320dba5d47a1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
